{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-03T18:15:36.881611Z",
     "start_time": "2025-11-03T18:15:32.107016Z"
    }
   },
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/code/Machine Learning/Projects/virtual-reception/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "/Users/dhruv/code/Machine Learning/Projects/virtual-reception/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b25dfe2f23180651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T18:15:38.472139Z",
     "start_time": "2025-11-03T18:15:38.469087Z"
    }
   },
   "source": [
    "PDF_FOLDER   = Path(\"../data/The Godfather Summary.pdf\")      # <-- change this\n",
    "EMBED_MODEL  = \"intfloat/e5-mistral-7b-instruct\"\n",
    "# GEN_MODEL    = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "GEN_MODEL    = \"mistral-3b\"\n",
    "CHUNK_SIZE   = 1000\n",
    "CHUNK_OVERLAP= 200"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "292495a59c351299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T18:15:40.658057Z",
     "start_time": "2025-11-03T18:15:40.553245Z"
    }
   },
   "source": [
    "def load_pdfs(pdf_path: Path):\n",
    "    docs = []\n",
    "    # Accept a single file or a folder\n",
    "    if pdf_path.is_file():\n",
    "        paths = [pdf_path]\n",
    "    else:\n",
    "        paths = list(pdf_path.glob(\"*.pdf\"))\n",
    "\n",
    "    for p in paths:\n",
    "        loader = PyPDFLoader(str(p))          # <-- PyPDFLoader in action\n",
    "        docs.extend(loader.load())\n",
    "    return docs\n",
    "\n",
    "raw_docs = load_pdfs(PDF_FOLDER)\n",
    "print(f\"Loaded {len(raw_docs)} pages from {PDF_FOLDER}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 pages from ../data/The Godfather Summary.pdf\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T18:15:42.536932Z",
     "start_time": "2025-11-03T18:15:42.533477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    ")\n",
    "chunks = text_splitter.split_documents(raw_docs)\n",
    "print(f\"Created {len(chunks)} chunks\")"
   ],
   "id": "7de53441d0ab0047",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 15 chunks\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-03T16:55:52.053462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})  # top-6 chunks"
   ],
   "id": "bdb1659fa321002",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [03:35<00:00, 108.00s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:35:15.514531Z",
     "start_time": "2025-11-03T14:35:14.623508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEN_MODEL,\n",
    "    # device_map=\"auto\",          # uses GPU if available, else CPU\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ],
   "id": "468989c84f8cd01",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 38.79it/s]\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.04 GiB, other allocations: 656.00 KiB, max allowed: 9.07 GiB). Tried to allocate 1.02 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      1\u001B[39m tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL, trust_remote_code=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      2\u001B[39m model = AutoModelForCausalLM.from_pretrained(\n\u001B[32m      3\u001B[39m     GEN_MODEL,\n\u001B[32m      4\u001B[39m     \u001B[38;5;66;03m# device_map=\"auto\",          # uses GPU if available, else CPU\u001B[39;00m\n\u001B[32m      5\u001B[39m     torch_dtype=\u001B[33m\"\u001B[39m\u001B[33mauto\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      6\u001B[39m     trust_remote_code=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m      7\u001B[39m )\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m pipe = \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtext-generation\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m300\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.95\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m llm = HuggingFacePipeline(pipeline=pipe)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/Machine Learning/Projects/virtual-reception/.venv/lib/python3.14/site-packages/transformers/pipelines/__init__.py:1229\u001B[39m, in \u001B[36mpipeline\u001B[39m\u001B[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[39m\n\u001B[32m   1226\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m processor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1227\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mprocessor\u001B[39m\u001B[33m\"\u001B[39m] = processor\n\u001B[32m-> \u001B[39m\u001B[32m1229\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpipeline_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframework\u001B[49m\u001B[43m=\u001B[49m\u001B[43mframework\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/Machine Learning/Projects/virtual-reception/.venv/lib/python3.14/site-packages/transformers/pipelines/text_generation.py:121\u001B[39m, in \u001B[36mTextGenerationPipeline.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    120\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    122\u001B[39m     \u001B[38;5;28mself\u001B[39m.check_model_type(\n\u001B[32m    123\u001B[39m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mtf\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001B[32m    124\u001B[39m     )\n\u001B[32m    125\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mprefix\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._preprocess_params:\n\u001B[32m    126\u001B[39m         \u001B[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001B[39;00m\n\u001B[32m    127\u001B[39m         \u001B[38;5;66;03m# as a \"default\".\u001B[39;00m\n\u001B[32m    128\u001B[39m         \u001B[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001B[39;00m\n\u001B[32m    129\u001B[39m         \u001B[38;5;66;03m# which is why we cannot put them in their respective methods.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/Machine Learning/Projects/virtual-reception/.venv/lib/python3.14/site-packages/transformers/pipelines/base.py:1044\u001B[39m, in \u001B[36mPipeline.__init__\u001B[39m\u001B[34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, device, binary_output, **kwargs)\u001B[39m\n\u001B[32m   1037\u001B[39m \u001B[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001B[39;00m\n\u001B[32m   1038\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   1039\u001B[39m     \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1040\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.model.device != \u001B[38;5;28mself\u001B[39m.device\n\u001B[32m   1041\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m.device, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.device < \u001B[32m0\u001B[39m)\n\u001B[32m   1042\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m hf_device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1043\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1044\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1046\u001B[39m \u001B[38;5;66;03m# If it's a generation pipeline and the model can generate:\u001B[39;00m\n\u001B[32m   1047\u001B[39m \u001B[38;5;66;03m# 1 - create a local generation config. This is done to avoid side-effects on the model as we apply local\u001B[39;00m\n\u001B[32m   1048\u001B[39m \u001B[38;5;66;03m# tweaks to the generation config.\u001B[39;00m\n\u001B[32m   1049\u001B[39m \u001B[38;5;66;03m# 2 - load the assistant model if it is passed.\u001B[39;00m\n\u001B[32m   1050\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pipeline_calls_generate \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.model.can_generate():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/Machine Learning/Projects/virtual-reception/.venv/lib/python3.14/site-packages/transformers/modeling_utils.py:4343\u001B[39m, in \u001B[36mPreTrainedModel.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   4338\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m dtype_present_in_args:\n\u001B[32m   4339\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   4340\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4341\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m `dtype` by passing the correct `dtype` argument.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4342\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m4343\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/Machine Learning/Projects/virtual-reception/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1371\u001B[39m, in \u001B[36mModule.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1368\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1369\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1371\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/Machine Learning/Projects/virtual-reception/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    934\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    935\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    940\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    941\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/Machine Learning/Projects/virtual-reception/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    934\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    935\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    940\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    941\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/Machine Learning/Projects/virtual-reception/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:957\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    953\u001B[39m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[32m    954\u001B[39m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[32m    955\u001B[39m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[32m    956\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m957\u001B[39m     param_applied = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    958\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001B[32m    960\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_subclasses\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfake_tensor\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FakeTensor\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/code/Machine Learning/Projects/virtual-reception/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1357\u001B[39m, in \u001B[36mModule.to.<locals>.convert\u001B[39m\u001B[34m(t)\u001B[39m\n\u001B[32m   1350\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t.dim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m):\n\u001B[32m   1351\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m t.to(\n\u001B[32m   1352\u001B[39m             device,\n\u001B[32m   1353\u001B[39m             dtype \u001B[38;5;28;01mif\u001B[39;00m t.is_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t.is_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1354\u001B[39m             non_blocking,\n\u001B[32m   1355\u001B[39m             memory_format=convert_to_format,\n\u001B[32m   1356\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1357\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1358\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1359\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1360\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1361\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1362\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1363\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) == \u001B[33m\"\u001B[39m\u001B[33mCannot copy out of meta tensor; no data!\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mRuntimeError\u001B[39m: MPS backend out of memory (MPS allocated: 9.04 GiB, other allocations: 656.00 KiB, max allowed: 9.07 GiB). Tried to allocate 1.02 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:35:20.561781Z",
     "start_time": "2025-11-03T14:35:20.543973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template = \"\"\"You are a helpful assistant. Answer the question using ONLY the provided context.\n",
    "If the context does not contain the answer, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ],
   "id": "b01628043ddb2cc7",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:35:22.793856Z",
     "start_time": "2025-11-03T14:35:22.738293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_context(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_context, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "f0d041e593e2b09e",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mformat_context\u001B[39m(docs):\n\u001B[32m      2\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.join(doc.page_content \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs)\n\u001B[32m      4\u001B[39m rag_chain = (\n\u001B[32m      5\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m\"\u001B[39m: retriever | format_context, \u001B[33m\"\u001B[39m\u001B[33mquestion\u001B[39m\u001B[33m\"\u001B[39m: RunnablePassthrough()}\n\u001B[32m      6\u001B[39m     | prompt\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     | \u001B[43mllm\u001B[49m\n\u001B[32m      8\u001B[39m     | StrOutputParser()\n\u001B[32m      9\u001B[39m )\n",
      "\u001B[31mNameError\u001B[39m: name 'llm' is not defined"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "question = \"What is the main conclusion of the paper about climate change?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "print(\"\\n--- ANSWER ---\")\n",
    "print(answer)"
   ],
   "id": "f963431c4e96d119"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
